# Abstract

LLM-based robots utilize Large Language Models (LLMs) as planners to translate natural language instructions into executable policies such as grasp(), move_to(), and open_gripper(). The inherent vulnerability of LLMs to jailbreak attacks extends the threat surface from generating malicious content to potentially executing harmful behaviors. However, our study reveals that existing jailbreak attempts against LLM-based robots primarily succeed at producing malicious-looking policies (intent jailbreaks) but often fail to induce harmful robot executions (behavior jailbreaks), as the generated policies are often non-executable or physically infeasible. In this paper, we systematically investigate this intent-behavior jailbreak gap to understand its root causes and inform effective defenses. Through a comprehensive measurement study, we find that the gap arises because current jailbreak methods overlook robot-specific syntax constraints and physical feasibility during both the policy optimization and evaluation processes. To bridge the gap, we introduce POEF (POlicy EFfective Jailbreak), an automated red-teaming framework. POEF leverages the hidden-layer gradients from unaligned LLMs and employs a multi-agent evaluator to generate harmful policies that are both syntactically valid and physically executable. Experiments on commercial robots and simulators show that POEF achieves an 80\% behavior jailbreak success rate and exhibits transferability across LLMs. Finally, we implement two defense strategies to mitigate such behavior jailbreak risks. Our work underscores the urgent need for robust countermeasures before the large-scale deployment of LLM-based robots in the real world.